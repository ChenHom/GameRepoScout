#!/usr/bin/env python3
"""
crawler.py
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ä»¥å¤šæ¢ GitHub Search Query å–å¾—å€™é¸éŠæˆ²å°ˆæ¡ˆæ¸…å–®ï¼Œä¸¦è¼¸å‡º `raw_repos.json / csv`.

â€£ .env è®Šæ•¸ä¸€è¦½
    GITHUB_TOKEN        GitHub Personal Access Tokenï¼ˆå»ºè­°å¿…å¡«ï¼‰
    QUERY_KEYWORDS      é—œéµå­—ï¼Œé è¨­ "unity game"
    MIN_STARS           æ˜Ÿæ•¸ä¸‹é™ï¼Œé è¨­ 20
    TOPIC_FILTERS       topic ç›¸é—œæ¢ä»¶
    LANG_FILTER         èªè¨€æ¢ä»¶
    DATE_FILTER         è¿‘æœŸæ›´æ–°æ¢ä»¶
    ANDROID_FILTER      Android å°ˆæ¡ˆç‰¹å¾µ (path:/AndroidManifest.xml)
    IOS_FILTER          iOS å°ˆæ¡ˆç‰¹å¾µ (filename:Info.plist)
    GRS_MAX_PAGES       æ¯æ¢ Query æœ€å¤šåˆ†é æ•¸ (REST Search ä¸Šé™ 10 = 1000 ç­†)

â€£ ç”¢å‡º
    output/raw_repos.json
    output/raw_repos.csv
"""

from __future__ import annotations
import json, os
from pathlib import Path
from typing import Dict, List

import pandas as pd
import requests
from dotenv import load_dotenv
from tqdm import tqdm

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ åŸºæœ¬è¨­å®š â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
load_dotenv()

TOKEN   = os.getenv("GITHUB_TOKEN")
HEADERS = {"Authorization": f"token {TOKEN}"} if TOKEN else {}

# å¯ç”± .env è¦†å¯«çš„æŸ¥è©¢æ¢ä»¶
QUERY_KEYWORDS = os.getenv("QUERY_KEYWORDS", "unity game")
MIN_STARS      = os.getenv("MIN_STARS", "20")

TOPIC_FILTERS  = os.getenv(
    "TOPIC_FILTERS",
    "",
)
LANG_FILTER    = os.getenv("LANG_FILTER", "")
DATE_FILTER    = os.getenv("DATE_FILTER", "pushed:>=2024-06-01")

ANDROID_FILTER = os.getenv("ANDROID_FILTER",  "")
IOS_FILTER     = os.getenv("IOS_FILTER",      "")

API_URL   = "https://api.github.com/search/repositories"
PER_PAGE  = 100
MAX_PAGES = int(os.getenv("GRS_MAX_PAGES", "10"))   # Search API æœ€å¤š 1,000 ç­†

# è¼¸å‡ºè·¯å¾‘
OUTPUT_DIR = Path("output"); OUTPUT_DIR.mkdir(exist_ok=True)
RAW_JSON   = OUTPUT_DIR / "raw_repos.json"
RAW_CSV    = OUTPUT_DIR / "raw_repos.csv"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ çµ„ Query æ¸…å–® â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BASE_QUERY = (
    f"{QUERY_KEYWORDS} {TOPIC_FILTERS} {LANG_FILTER} "
    f"stars:>={MIN_STARS} {DATE_FILTER}"
)

QUERY_LIST: List[str] = []
if ANDROID_FILTER:
    QUERY_LIST.append(f"{BASE_QUERY} {ANDROID_FILTER}")
if IOS_FILTER:
    QUERY_LIST.append(f"{BASE_QUERY} {IOS_FILTER}")
if not QUERY_LIST:                             # è‹¥å…©è€…çš†ç©ºï¼Œè·‘ base
    QUERY_LIST.append(BASE_QUERY)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ å·¥å…·å‡½å¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_page(q: str, page: int) -> Dict:
    params = {
        "q": q,
        "sort": "stars",
        "order": "desc",
        "per_page": PER_PAGE,
        "page": page,
    }
    resp = requests.get(API_URL, headers=HEADERS, params=params, timeout=30)
    print(
        f"[DEBUG] page={page} status={resp.status_code} "
        f"remaining={resp.headers.get('X-RateLimit-Remaining')}"
    )
    print(f"[DEBUG] full_url: {resp.url}")
    resp.raise_for_status()
    return resp.json()

def run_query(q: str) -> List[Dict]:
    data  = fetch_page(q, 1)
    items = data.get("items", [])
    total = min(data.get("total_count", 0), MAX_PAGES * PER_PAGE)

    if total > PER_PAGE:
        pages = min((total + PER_PAGE - 1) // PER_PAGE, MAX_PAGES)
        for p in tqdm(range(2, pages + 1), desc="paging"):
            items.extend(fetch_page(q, p).get("items", []))
    return items

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ä¸»ç¨‹å¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    all_items: List[Dict] = []
    for q in QUERY_LIST:
        print(f"\nğŸ”  Query => {q}")
        all_items.extend(run_query(q))

    # å»é‡
    unique_items = {i["full_name"]: i for i in all_items}.values()
    print(f"âœ…  Total raw repos (dedup): {len(unique_items)}")

    # å­˜ JSON
    RAW_JSON.write_text(json.dumps(list(unique_items), ensure_ascii=False, indent=2))

    # å­˜ CSV
    df = pd.DataFrame({
        "repo_name": [r["full_name"] for r in unique_items],
        "html_url":  [r["html_url"]  for r in unique_items],
        "stars":     [r["stargazers_count"] for r in unique_items],
        "pushed_at": [r["pushed_at"][:10]   for r in unique_items],
        "license":   [(r.get("license") or {}).get("spdx_id", "N/A") for r in unique_items],
    })
    df.to_csv(RAW_CSV, index=False)
    print(f"ğŸ“„  raw_repos.json / raw_repos.csv å·²å¯«å…¥ {OUTPUT_DIR}")
